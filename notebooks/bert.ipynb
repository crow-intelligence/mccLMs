{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# huBERT\n",
    "A Hungarian BERT model. More on huBERT info [here](https://hlt.bme.hu/en/resources/hubert)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import warnings\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "hf_logging.set_verbosity_error()\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import AutoTokenizer, AutoModel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BERT word embeddings\n",
    "Getting word embeddings from BERT."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# source: https://discuss.huggingface.co/t/generate-raw-word-embeddings-using-transformer-models-like-bert-for-downstream-process/2958/2\n",
    "def get_word_idx(sent: str, word: str):\n",
    "    return sent.split(\" \").index(word)\n",
    "\n",
    "\n",
    "def get_hidden_states(encoded, token_ids_word, model, layers):\n",
    "     \"\"\"Push input IDs through model. Stack and sum `layers` (last four by default).\n",
    "        Select only those subword token outputs that belong to our word of interest\n",
    "        and average them.\"\"\"\n",
    "     with torch.no_grad():\n",
    "         output = model(**encoded)\n",
    "\n",
    "     # Get all hidden states\n",
    "     states = output.hidden_states\n",
    "     # Stack and sum all requested layers\n",
    "     output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
    "     # Only select the tokens that constitute the requested word\n",
    "     word_tokens_output = output[token_ids_word]\n",
    "\n",
    "     return word_tokens_output.mean(dim=0)\n",
    "\n",
    "\n",
    "def get_word_vector(sent, idx, tokenizer, model, layers):\n",
    "     \"\"\"Get a word vector by first tokenizing the input sentence, getting all token idxs\n",
    "        that make up the word of interest, and then `get_hidden_states`.\"\"\"\n",
    "     encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n",
    "     # get all token idxs that belong to the word of interest\n",
    "     token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n",
    "\n",
    "     return get_hidden_states(encoded, token_ids_word, model, layers)\n",
    "\n",
    "\n",
    "def wvs_from_sent(sent, layers=None):\n",
    "     # Use last four layers by default\n",
    "     layers = [-4, -3, -2, -1] if layers is None else layers\n",
    "     tokenizer = AutoTokenizer.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\")\n",
    "     model = AutoModel.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\", output_hidden_states=True)\n",
    "\n",
    "     indices = [get_word_idx(sent, wd.strip()) for wd in sent.split()]\n",
    "     wvs = [get_word_vector(sent, wid, tokenizer, model, layers) for wid in indices]\n",
    "     return sent.split(), wvs\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "wds1, wvs1 = wvs_from_sent(\"Ez egy szép kis példa\")\n",
    "wds2, wvs2 = wvs_from_sent(\"Itt pedig egy másik példa\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see if the word embeddings are the same."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.equal(wvs1[1], wvs2[2])) # egy\n",
    "print(torch.equal(wvs1[-1], wvs2[-1])) # példa"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Word embeddings extracted from the model are context sensitive!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hungarian folk tales\n",
    "Let's create vectors for each word in our corpus. (This might take some time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# don't run this on colab, it will go through the whole corpus\n",
    "wd2vc = {}\n",
    "i = 0\n",
    "with open(\"../data/processed/lemmatized.txt\", \"r\") as infile:\n",
    "     for l in infile:\n",
    "          wds, wvs = wvs_from_sent(l.strip().lower())\n",
    "          for k, v in dict(zip(wds, wvs)).items():\n",
    "               if k not in wd2vc:\n",
    "                    if len(v.numpy()) == 768:\n",
    "                         wd2vc[k] = v.numpy()\n",
    "               else:\n",
    "                    try:\n",
    "                         mean_vec = np.mean([wd2vc[k], v.numpy()], axis=1)\n",
    "                         if len(mean_vec) == 768:\n",
    "                              wd2vc[k] = mean_vec\n",
    "                    except Exception as e:\n",
    "                         continue\n",
    "          i += 1\n",
    "          print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../models/bert_mese.pkl\", \"wb\") as outfile:\n",
    "     pickle.dump(wd2vc, outfile)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def dist(wd1, wd2):\n",
    "     assert wd1 in wd2vc\n",
    "     assert wd2 in wd2vc\n",
    "     return cosine(wd2vc[wd1], wd2vc[wd2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019718647003173828\n"
     ]
    }
   ],
   "source": [
    "print(dist(\"a\", \"az\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Most similar"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def similar(wd, n):\n",
    "     assert wd in wd2vc\n",
    "     w_vector = wd2vc[wd]\n",
    "     vocabulary = list(wd2vc.keys())\n",
    "     m = np.asarray([np.asarray(e) for e in wd2vc.values()])\n",
    "     sims = list(m.dot(w_vector))\n",
    "     most_similar_values = heapq.nlargest(n + 10, sims)\n",
    "     most_similar_indices = [sims.index(e) for e in list(most_similar_values)]\n",
    "     most_similar_words = [vocabulary[e] for e in most_similar_indices]\n",
    "     if wd in most_similar_words:\n",
    "          most_similar_words.remove(wd)\n",
    "     return most_similar_words[:n]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "['az', 'magyar', 'király']"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar(\"a\", 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analogy\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def similar(positive, negative, topn=3):\n",
    "   \"\"\"Analogy difference\"\"\"\n",
    "   assert positive[0] in wd2vc\n",
    "   assert positive[1] in wd2vc\n",
    "   assert negative in wd2vc\n",
    "\n",
    "   pos1_vector = wd2vc[positive[0]]\n",
    "   pos2_vector = wd2vc[positive[1]]\n",
    "   neg_vector = wd2vc[negative]\n",
    "   target_vector = np.subtract(neg_vector, np.add(pos1_vector, pos2_vector))\n",
    "   vocabulary = list(wd2vc.keys())\n",
    "   m = np.asarray([np.asarray(e) for e in wd2vc.values()])\n",
    "   sims = list(m.dot(target_vector))\n",
    "\n",
    "   most_similar_values = heapq.nlargest(topn + 10, sims)\n",
    "   most_similar_indices = [sims.index(e) for e in list(most_similar_values)]\n",
    "   most_similar_words = [vocabulary[e] for e in most_similar_indices]\n",
    "\n",
    "   if positive[0] in most_similar_words:\n",
    "       most_similar_words.remove(positive[0])\n",
    "   if positive[1] in most_similar_words:\n",
    "       most_similar_words.remove(positive[1])\n",
    "   if negative in most_similar_words:\n",
    "       most_similar_words.remove(negative)\n",
    "   if len(most_similar_words) > 0:\n",
    "       return most_similar_words[:topn]\n",
    "   else:\n",
    "       return []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "['gyémánt', 'kakas', 'kis']"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar([\"a\", \"az\"], \"őzike\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kakukktojás"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def doesnt_match(lst):\n",
    "    \"\"\"odd-one-out\"\"\"\n",
    "    vocabulary = list(wd2vc.keys())\n",
    "    word_idxs = [vocabulary.index(wd) for wd in lst]\n",
    "    m = np.asarray([np.asarray(e) for e in wd2vc.values()])\n",
    "\n",
    "    word_vectors = np.vstack(m[i] for i in word_idxs)\n",
    "    mean = np.mean(word_vectors, axis=0)\n",
    "    dists = [abs(cosine(e, mean)) for e in word_vectors]\n",
    "    mdist = max(dists)\n",
    "    midx = dists.index(mdist)\n",
    "    return lst[midx]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "népmesék\n"
     ]
    }
   ],
   "source": [
    "print(doesnt_match(['magyar', 'népmesék', 'arany', 'lászló']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "+ it is designed for a different task!\n",
    "+ it is a bit complicated to handle it\n",
    "+ it gives very good context sensitive embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
